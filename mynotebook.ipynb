{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import  numpy as np\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from utils import EarlyStopping, LRScheduler\n",
    "from models.Captioner import Encoder, DecoderWithAttention, Decoder_NoTFNoAttNoGlove\n",
    "from data import CaptionDataset\n",
    "from embedding import get_embedding_matrix\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data parameters\n",
    "train_file = '/home/gaurangajitk/DL/data/image-caption-data/annotations_train1.txt'\n",
    "val_file = '/home/gaurangajitk/DL/data/image-caption-data/annotations_val.txt'\n",
    "data_folder = '/home/gaurangajitk/DL/data/image-caption-data'  # folder with data files saved by create_input_files.py\n",
    "data_name = '5_cap_per_img_15_min_word_freq'  # base name shared by data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param data_name: base name of processed datasets\n",
    "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
    "        :param transform: image transform pipeline\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
    "\n",
    "        # Open hdf5 file where images are stored\n",
    "        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
    "        self.imgs = self.h['images']\n",
    "\n",
    "        # Captions per image\n",
    "        self.cpi = self.h.attrs['captions_per_image']\n",
    "\n",
    "        # Load encoded captions (completely into memory)\n",
    "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "\n",
    "        # Load caption lengths (completely into memory)\n",
    "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
    "            self.caplens = json.load(j)\n",
    "        \n",
    "        # Load meta image IDs (completely into memory)\n",
    "        with open(os.path.join(data_folder, self.split + '_META_IMAGES_' + data_name + '.json'), 'r') as j:\n",
    "            self.meta_im = json.load(j)\n",
    "        \n",
    "        # if split == 'VAL':\n",
    "        #     self.imgs = self.imgs[1000:1500]\n",
    "        #     self.captions = self.captions[1000:1500]\n",
    "        #     self.caplens = self.caplens[1000:1500]\n",
    "\n",
    "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.captions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
    "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "\n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "\n",
    "        if self.split == 'TRAIN':\n",
    "            return img, caption, caplen\n",
    "        else:\n",
    "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
    "            return img, caption, caplen, all_captions, self.meta_im[i // self.cpi]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Datasets\n"
     ]
    }
   ],
   "source": [
    "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "config = {\n",
    "        'encoded_image_size': 14,\n",
    "        'fine_tune_encoder': False,\n",
    "        'attention_dim': 512,\n",
    "        'embed_dim': 200,\n",
    "        'decoder_dim': 512,\n",
    "        'dropout': 0.5,\n",
    "        'encoder_dim': 2048,\n",
    "        'encoder_lr': 1e-4,\n",
    "        'decoder_lr': 1e-3,\n",
    "        'batch_size': 10\n",
    "    }\n",
    "\n",
    "print('Preparing Datasets')\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),    #add random flip transform later\n",
    "        ])\n",
    "\n",
    "trainset = CaptionDataset(data_folder, data_name, 'TRAIN', transform=data_transforms)\n",
    "valset = CaptionDataset(data_folder, data_name, 'VAL', transform=data_transforms)\n",
    "testset = CaptionDataset(data_folder, data_name, 'TEST', transform=data_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'], shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=config['batch_size'], shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=config['batch_size'], shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VizWiz_val_00000001.jpg'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset[9][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VizWiz_val_00000888.jpg',\n",
       " 'VizWiz_val_00000406.jpg',\n",
       " 'VizWiz_val_00001111.jpg',\n",
       " 'VizWiz_val_00000292.jpg',\n",
       " 'VizWiz_val_00000080.jpg',\n",
       " 'VizWiz_val_00001121.jpg',\n",
       " 'VizWiz_val_00001269.jpg',\n",
       " 'VizWiz_val_00000787.jpg',\n",
       " 'VizWiz_val_00000550.jpg',\n",
       " 'VizWiz_val_00000370.jpg')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creare Model\n"
     ]
    }
   ],
   "source": [
    "print('Creare Model')\n",
    "encoder = Encoder(config)\n",
    "decoder = dec(config, vocab_size=len(word_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained glove embeddidngs\n",
      "Pre-processing /home/gaurangajitk/DL/data/image-caption-data/glove.6B.200d.txt vectors...\n",
      "2821 / 2849 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "print('Load pretrained glove embeddidngs')\n",
    "word_embedding = get_embedding_matrix(word_map.keys())\n",
    "decoder.load_pretrained_embeddings(word_embedding)\n",
    "decoder.fine_tune_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1739/3985849334.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  meta_cap = np.array(self.meta_cap[i // self.cpi]).reshape(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 (1, 5) ['VizWiz_val_00000950.jpg'] [[list(['a', 'black', 'laptop', 'computer', 'showing', 'a', 'page', 'of', 'computer', 'organization', 'and', 'design'])\n",
      "  list(['blue', 'book', 'cover', 'of', 'hardware', 'and', 'software', 'textbook'])\n",
      "  list(['a', 'pdf', 'of', 'a', 'book', 'on', 'a', 'computer', 'screen', 'about', 'computer', 'organization', 'and', 'design'])\n",
      "  list(['an', 'image', 'of', 'a', 'book', 'on', 'computer', 'organization', 'and', 'design', 'with', 'its', 'front', 'and', 'back', 'cover', 'facing', 'out', 'is', 'presented', 'on', 'the', 'screen', 'of', 'a', 'computer'])\n",
      "  list(['a', 'browser', 'window', 'shows', 'a', 'book', 'cover', 'that', 'is', 'a', 'scanned', 'image'])]]\n",
      "1500 (1, 5) ['VizWiz_val_00000105.jpg'] [[list(['a', 'screenshot', 'of', 'a', 'computer', 'monitor', 'showing', 'an', 'error', 'for', 'no', 'device', 'drivers', 'being', 'found'])\n",
      "  list(['a', 'windows', 'computer', 'message', 'that', 'says', 'no', 'device', 'drivers', 'were', 'found'])\n",
      "  list(['a', 'laptop', 'screen', 'showing', 'a', 'no', 'device', 'drivers', 'alert'])\n",
      "  list(['a', 'windows', 'pc', 'error', 'message', 'that', 'says', 'no', 'device', 'drivers', 'found'])\n",
      "  list(['a', 'computer', 'screen', 'asking', 'to', 'select', 'the', 'driver', 'to', 'be', 'installed', 'however', 'there', 'is', 'an', 'error', 'screen', 'saying', 'no', 'device', 'drivers', 'were', 'found'])]]\n",
      "1500 (1, 5) ['VizWiz_val_00000753.jpg'] [[list(['pictured', 'is', 'the', 'back', 'of', 'a', 'persons', 'head', 'and', 'another', 'persons', 'torso', 'and', 'hand'])\n",
      "  list(['the', 'back', 'of', 'a', 'man', 'wearing', 'a', 'polo', 'shirt', 'standing', 'in', 'front', 'of', 'two', 'other', 'people'])\n",
      "  list(['a', 'boy', 'in', 'a', 'grey', 'shirt', 'has', 'his', 'back', 'turned', 'to', 'the', 'camera', 'as', 'he', 'sits', 'with', 'a', 'few', 'other', 'people'])\n",
      "  list(['a', 'man', 'with', 'a', 'gray', 'shirt', 'sitting', 'on', 'the', 'floor', 'with', 'other', 'people'])\n",
      "  list(['an', 'person', 'in', 'grey', 'shirt', 'leans', 'forward', 'as', 'he', 'discusses', 'with', 'his', 'group'])]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 62, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# train_batch = iter(train_loader)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000007vscode-remote?line=1'>2</a>\u001b[0m val_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(val_loader))\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1200'>1201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1201'>1202</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1202'>1203</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1226'>1227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1227'>1228</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1228'>1229</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1229'>1230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=429'>430</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=430'>431</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=431'>432</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=432'>433</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=433'>434</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 62, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n"
     ]
    }
   ],
   "source": [
    "# train_batch = iter(train_loader)\n",
    "val_batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VizWiz_val_00000663.jpg',)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, captions, cap_len = train_batch\n",
    "encoder_out = encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, encoded_captions, decode_lengths, alphas, sort_ind = decoder(encoder_out, captions, cap_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 2849])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pack_padded_sequence(predictions, decode_lengths, batch_first=True)\n",
    "targets = pack_padded_sequence(encoded_captions, decode_lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([423, 2849]) torch.Size([423])\n"
     ]
    }
   ],
   "source": [
    "print(a.data.shape, targets.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ind = a.data.topk(2, 1, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = targets.data.view(-1, 1).expand_as(ind).eq(targets.data.view(-1, 1).expand_as(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(846.)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct.view(-1).float().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = encoder_out.size(0)\n",
    "encoder_dim = encoder_out.size(-1)  #2048\n",
    "vocab_size = decoder.vocab_size\n",
    "\n",
    "# Flatten image\n",
    "encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "num_pixels = encoder_out.size(1)\n",
    "\n",
    "# Sort input data by decreasing lengths; why? apparent below\n",
    "caption_lengths, sort_ind = cap_len.squeeze(1).sort(dim=0, descending=True)\n",
    "encoder_out = encoder_out[sort_ind]\n",
    "encoded_captions = captions[sort_ind]\n",
    "\n",
    "# Embedding\n",
    "embeddings = decoder.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "# Initialize LSTM state\n",
    "h, c = decoder.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "# We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "# So, decoding lengths are actual lengths - 1\n",
    "decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "# Create tensors to hold word predicion scores and alphas\n",
    "predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size)\n",
    "alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels)\n",
    "\n",
    "# At each time-step, decode by\n",
    "# attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "# then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "for t in range(min(1, max(decode_lengths))):\n",
    "    batch_size_t = sum([l > t for l in decode_lengths])\n",
    "    attention_weighted_encoding, alpha = decoder.attention(encoder_out[:batch_size_t],\n",
    "                                                        h[:batch_size_t])\n",
    "    gate = decoder.sigmoid(decoder.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "    attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "    h, c = decoder.decode_step(\n",
    "        torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "        (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "    preds = decoder.fc(decoder.dropout(h))  # (batch_size_t, vocab_size)\n",
    "    predictions[:batch_size_t, t, :] = preds\n",
    "    alphas[:batch_size_t, t, :] = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 2849])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 102])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, caps, caplens, allcaps = val_batch\n",
    "allcaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcaps = allcaps[sort_ind]\n",
    "references = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(allcaps.shape[0]):\n",
    "    img_caps = allcaps[j].tolist()\n",
    "    img_captions = list(\n",
    "        map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
    "            img_caps))  # remove <start> and pads\n",
    "    references.append(img_captions)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(references[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = torch.max(predictions, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.tolist()\n",
    "temp_preds = list()\n",
    "for j, p in enumerate(preds):\n",
    "    temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
    "preds = temp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/gaurangajitk/DL/image-captioning/data.py\", line 58, in __getitem__\n    all_captions = torch.LongTensor(\nValueError: expected sequence of length 157 at dim 1 (got 102)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(val_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000022vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1200'>1201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1201'>1202</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1202'>1203</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1226'>1227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1227'>1228</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1228'>1229</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1229'>1230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=429'>430</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=430'>431</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=431'>432</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=432'>433</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/_utils.py?line=433'>434</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/gaurangajitk/anaconda3/envs/cascades/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/gaurangajitk/DL/image-captioning/data.py\", line 58, in __getitem__\n    all_captions = torch.LongTensor(\nValueError: expected sequence of length 157 at dim 1 (got 102)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(val_loader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, 'VAL' + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
    "    captions = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "4430",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4429\u001b[39m,\u001b[39mlen\u001b[39m(captions)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000024vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(captions[i]) \u001b[39m==\u001b[39m \u001b[39m102\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 4430"
     ]
    }
   ],
   "source": [
    "for i in range(4429,len(captions)):\n",
    "    assert len(captions[i]) == 102, f'{i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset.imgs[4430].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 157 at dim 1 (got 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Barc_cascades/home/gaurangajitk/DL/image-captioning/mynotebook.ipynb#ch0000026vscode-remote?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(valset[\u001b[39m4430\u001b[39;49m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/DL/image-captioning/data.py:58\u001b[0m, in \u001b[0;36mCaptionDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gaurangajitk/DL/image-captioning/data.py?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img, caption, caplen\n\u001b[1;32m     <a href='file:///home/gaurangajitk/DL/image-captioning/data.py?line=55'>56</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/gaurangajitk/DL/image-captioning/data.py?line=56'>57</a>\u001b[0m     \u001b[39m# For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gaurangajitk/DL/image-captioning/data.py?line=57'>58</a>\u001b[0m     all_captions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mLongTensor(\n\u001b[1;32m     <a href='file:///home/gaurangajitk/DL/image-captioning/data.py?line=58'>59</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptions[((i \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpi) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpi):(((i \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpi) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpi) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpi)])\n\u001b[1;32m     <a href='file:///home/gaurangajitk/DL/image-captioning/data.py?line=59'>60</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img, caption, caplen, all_captions\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 157 at dim 1 (got 102)"
     ]
    }
   ],
   "source": [
    "plt.imshow(valset[4430][0].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38250"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valset.caplens)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ee9683fdfa7b35d9efa03fe6c9a658d126a8681914d3f8a30cd0a7db021a5f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cascades')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
